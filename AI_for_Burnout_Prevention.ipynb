{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "udMLQTiItyKC",
        "outputId": "77b31d70-5277-4372-ae21-b1b520bd2e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install LangChain, LangGraph, Gemini SDK\n",
        "!pip install -q langchain langgraph google-generativeai langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all dependencies\n",
        "import os\n",
        "import getpass\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "0rFiXKIot5sx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Secure input for Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Gemini API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sllH3EWFuu-n",
        "outputId": "791c6b9d-4ebd-48e7-9c19-daf38b819c0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Gemini API key: 路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Gemini Pro with LangChain wrapper\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)"
      ],
      "metadata": {
        "id": "dxQ7F4WbuwKZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate burnout based on user inputs\n",
        "def evaluate_burnout_risk(mood: int, sleep: float, breaks: float) -> str:\n",
        "    risk_flags = 0\n",
        "    if mood <= 3:\n",
        "        risk_flags += 1\n",
        "    if sleep < 6:\n",
        "        risk_flags += 1\n",
        "    if breaks < 1:\n",
        "        risk_flags += 1\n",
        "    return \"high\" if risk_flags >= 2 else \"low\""
      ],
      "metadata": {
        "id": "vB9snSEqwQjZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "# Define the state schema\n",
        "class State(TypedDict):\n",
        "    mood: int\n",
        "    sleep_hours: float\n",
        "    break_hours: float\n",
        "    risk_level: str\n",
        "    recommendations: str\n",
        "    affirmation: str\n",
        "\n",
        "# Start Node\n",
        "def start_node(state: State) -> State:\n",
        "    print(\" Hello! I'm your self-care AI assistant.\")\n",
        "    print(\"Let's check how you're doing today.\\n\")\n",
        "\n",
        "    # Simulated inputs (replace with actual input for interactive use)\n",
        "    mood = int(input(\"How would you rate your mood today (1 = bad, 5 = great)? \"))\n",
        "    sleep_hours = float(input(\"How many hours did you sleep last night? \"))\n",
        "    break_hours = float(input(\"How many hours did you take proper breaks today? \"))\n",
        "\n",
        "    return {\"mood\": mood, \"sleep_hours\": sleep_hours, \"break_hours\": break_hours}"
      ],
      "metadata": {
        "id": "smrddkZd15Y9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzer node\n",
        "def analyzer_node(state: State) -> State:\n",
        "    mood = state.get(\"mood\", 3)\n",
        "    sleep = state.get(\"sleep_hours\", 6.0)\n",
        "    breaks = state.get(\"break_hours\", 1.0)\n",
        "\n",
        "    risk = evaluate_burnout_risk(mood, sleep, breaks)\n",
        "    print(f\" Burnout Risk Level: {risk.upper()}\")\n",
        "\n",
        "    return {\"risk_level\": risk}"
      ],
      "metadata": {
        "id": "SnbY_IP31_S4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses Gemini to suggest 3 simple self-care activities\n",
        "def recommendation_node(state: State) -> State:\n",
        "    mood = state[\"mood\"]\n",
        "    sleep = state[\"sleep_hours\"]\n",
        "    breaks = state[\"break_hours\"]\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You're a self-care advisor for university students.\"),\n",
        "        (\"human\", f\"\"\"Suggest 3 simple, practical self-care activities for a student who is showing signs of burnout:\n",
        "              Mood rating: {mood}\n",
        "              Sleep hours: {sleep}\n",
        "              Breaks today: {breaks}\n",
        "              Be gentle, specific, and encouraging.\"\"\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({})\n",
        "    suggestion = response.content.strip()\n",
        "\n",
        "    print(\"\\n Self-Care Recommendations:\\n\")\n",
        "    print(suggestion)\n",
        "    return state\n",
        "    #return {\"recommendations\": suggestion}\n"
      ],
      "metadata": {
        "id": "ONWXvrFK2A_m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def affirmation_node(state: State) -> State:\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You're a motivational self-care coach.\"),\n",
        "        (\"human\", \"The student is doing well and not at burnout risk. Write a short, encouraging affirmation.\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({})\n",
        "    affirmation = response.content.strip()\n",
        "\n",
        "    print(\"\\n Affirmation:\\n\")\n",
        "    print(affirmation)\n",
        "\n",
        "    return state\n",
        "    #return {\"affirmation\": affirmation}"
      ],
      "metadata": {
        "id": "RUzMfJ9W2C2-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def router_node(state: State) -> str:\n",
        "    return \"recommendation\" if state[\"risk_level\"] == \"high\" else \"affirmation\""
      ],
      "metadata": {
        "id": "WMMdj2Vx2EYx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LangGraph flow using StateGraph\n",
        "builder = StateGraph(State)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"start\", start_node)\n",
        "builder.add_node(\"analyzer\", analyzer_node)\n",
        "#builder.add_node(\"router\", router_node)\n",
        "builder.add_node(\"recommendation\", recommendation_node)\n",
        "builder.add_node(\"affirmation\", affirmation_node)\n",
        "\n",
        "# Define edges\n",
        "builder.set_entry_point(\"start\")\n",
        "builder.add_edge(\"start\", \"analyzer\")\n",
        "#builder.add_edge(\"analyzer\", \"router\")\n",
        "builder.add_conditional_edges(\"analyzer\", router_node, {\n",
        "    \"recommendation\": \"recommendation\",\n",
        "    \"affirmation\": \"affirmation\"\n",
        "})\n",
        "\n",
        "\n",
        "builder.add_edge(\"recommendation\", END)\n",
        "builder.add_edge(\"affirmation\", END)\n",
        "\n",
        "# Compile graph\n",
        "app = builder.compile()\n"
      ],
      "metadata": {
        "id": "4ygkaZl22Fys"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the graph\n",
        "final_state = app.invoke({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiekcP-h2Iw2",
        "outputId": "5a28c145-ca94-4ddc-a79d-28e4b13ef594"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello! I'm your self-care AI assistant.\n",
            "Let's check how you're doing today.\n",
            "\n",
            "How would you rate your mood today (1 = bad, 5 = great)? 4\n",
            "How many hours did you sleep last night? 7\n",
            "How many hours did you take proper breaks today? 2\n",
            " Burnout Risk Level: LOW\n",
            "\n",
            " Affirmation:\n",
            "\n",
            "You're thriving!  Keep shining brightly and nurturing your amazing progress.  You've got this!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxZr3-c22LkX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}