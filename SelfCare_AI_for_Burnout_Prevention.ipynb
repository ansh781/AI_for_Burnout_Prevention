{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjOsd4S+w35kNPF+fsjoX5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansh781/AI_for_Burnout_Prevention/blob/main/SelfCare_AI_for_Burnout_Prevention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMNqQ0kN1ztv",
        "outputId": "5e01c584-7c99-48e5-dd16-496447f4b76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install LangChain, LangGraph, Gemini SDK\n",
        "!pip install -q langchain langgraph google-generativeai langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n"
      ],
      "metadata": {
        "id": "OqNI4HM12Oqo"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Secure input for Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Gemini API key: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bsb_mon2YiT",
        "outputId": "ec39440a-dab9-4b79-f7e6-22ac5e0625b6"
      },
      "execution_count": 98,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Gemini API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Gemini Pro with LangChain wrapper\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)\n"
      ],
      "metadata": {
        "id": "FzXTIFL72sAS"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate burnout based on user inputs\n",
        "def evaluate_burnout_risk(mood: int, sleep: float, breaks: float) -> str:\n",
        "    risk_flags = 0\n",
        "    if mood <= 3:\n",
        "        risk_flags += 1\n",
        "    if sleep < 6:\n",
        "        risk_flags += 1\n",
        "    if breaks < 1:\n",
        "        risk_flags += 1\n",
        "    return \"high\" if risk_flags >= 2 else \"low\""
      ],
      "metadata": {
        "id": "8KupWcBA2ymV"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "# Define the state schema\n",
        "class State(TypedDict):\n",
        "    mood: int\n",
        "    sleep_hours: float\n",
        "    break_hours: float\n",
        "    risk_level: str\n",
        "    recommendations: str\n",
        "    affirmation: str\n",
        "\n",
        "# --- Start Node (Greeting + Data Collection Prompt) ---\n",
        "def start_node(state: State) -> State:\n",
        "    print(\"👋 Hello! I'm your self-care AI assistant.\")\n",
        "    print(\"Let's check how you're doing today.\\n\")\n",
        "\n",
        "    # Simulated inputs (replace with actual input for interactive use)\n",
        "    mood = int(input(\"How would you rate your mood today (1 = bad, 5 = great)? \"))\n",
        "    sleep_hours = float(input(\"How many hours did you sleep last night? \"))\n",
        "    break_hours = float(input(\"How many hours did you take proper breaks today? \"))\n",
        "\n",
        "    return {\"mood\": mood, \"sleep_hours\": sleep_hours, \"break_hours\": break_hours}"
      ],
      "metadata": {
        "id": "ckFO_HNw25ne"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzer node\n",
        "def analyzer_node(state: State) -> State:\n",
        "    mood = state.get(\"mood\", 3)\n",
        "    sleep = state.get(\"sleep_hours\", 6.0)\n",
        "    breaks = state.get(\"break_hours\", 1.0)\n",
        "\n",
        "    risk = evaluate_burnout_risk(mood, sleep, breaks)\n",
        "    print(f\"📊 Burnout Risk Level: {risk.upper()}\")\n",
        "\n",
        "    return {\"risk_level\": risk}"
      ],
      "metadata": {
        "id": "GKUi8HTf3FVF"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses Gemini to suggest 3 simple self-care activities\n",
        "def recommendation_node(state: State) -> State:\n",
        "    mood = state[\"mood\"]\n",
        "    sleep = state[\"sleep_hours\"]\n",
        "    breaks = state[\"break_hours\"]\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You're a self-care advisor for university students.\"),\n",
        "        (\"human\", f\"\"\"Suggest 3 simple, practical self-care activities for a student who is showing signs of burnout:\n",
        "              Mood rating: {mood}\n",
        "              Sleep hours: {sleep}\n",
        "              Breaks today: {breaks}\n",
        "              Be gentle, specific, and encouraging.\"\"\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({})\n",
        "    suggestion = response.content.strip()\n",
        "\n",
        "    print(\"\\n🧘 Self-Care Recommendations:\\n\")\n",
        "    print(suggestion)\n",
        "    return state\n",
        "    #return {\"recommendations\": suggestion}\n"
      ],
      "metadata": {
        "id": "TKiF9SN33crI"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def affirmation_node(state: State) -> State:\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You're a motivational self-care coach.\"),\n",
        "        (\"human\", \"The student is doing well and not at burnout risk. Write a short, encouraging affirmation.\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({})\n",
        "    affirmation = response.content.strip()\n",
        "\n",
        "    print(\"\\n🌟 Affirmation:\\n\")\n",
        "    print(affirmation)\n",
        "\n",
        "    return state\n",
        "    #return {\"affirmation\": affirmation}"
      ],
      "metadata": {
        "id": "n9TlGsmE4Gqj"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def router_node(state: State) -> str:\n",
        "    return \"recommendation\" if state[\"risk_level\"] == \"high\" else \"affirmation\""
      ],
      "metadata": {
        "id": "T9Vlx0XnIU9m"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LangGraph flow using StateGraph\n",
        "builder = StateGraph(State)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"start\", start_node)\n",
        "builder.add_node(\"analyzer\", analyzer_node)\n",
        "#builder.add_node(\"router\", router_node)\n",
        "builder.add_node(\"recommendation\", recommendation_node)\n",
        "builder.add_node(\"affirmation\", affirmation_node)\n",
        "\n",
        "# Define edges\n",
        "builder.set_entry_point(\"start\")\n",
        "builder.add_edge(\"start\", \"analyzer\")\n",
        "#builder.add_edge(\"analyzer\", \"router\")\n",
        "builder.add_conditional_edges(\"analyzer\", router_node, {\n",
        "    \"recommendation\": \"recommendation\",\n",
        "    \"affirmation\": \"affirmation\"\n",
        "})\n",
        "\n",
        "\n",
        "builder.add_edge(\"recommendation\", END)\n",
        "builder.add_edge(\"affirmation\", END)\n",
        "\n",
        "# Compile graph\n",
        "app = builder.compile()\n"
      ],
      "metadata": {
        "id": "l3v9B1C55XBt"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Invoke the graph\n",
        "final_state = app.invoke({})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnyiEuWl5W-G",
        "outputId": "33f05591-2333-436a-ff6e-0dab4bb6a791"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👋 Hello! I'm your self-care AI assistant.\n",
            "Let's check how you're doing today.\n",
            "\n",
            "How would you rate your mood today (1 = bad, 5 = great)? 3\n",
            "How many hours did you sleep last night? 3\n",
            "How many hours did you take proper breaks today? 3\n",
            "📊 Burnout Risk Level: HIGH\n",
            "\n",
            "🧘 Self-Care Recommendations:\n",
            "\n",
            "It sounds like you're feeling the pressure, and that's completely understandable for a student.  A mood rating of 3, limited sleep, and only three breaks isn't ideal, but it's a great starting point for making positive changes. Let's focus on some gentle, manageable self-care to help you recharge.\n",
            "\n",
            "Here are three simple, practical activities you can incorporate today:\n",
            "\n",
            "1. **Mindful Movement Break (10 minutes):**  Instead of scrolling on your phone during a break, try a short mindful movement session.  This doesn't have to be a vigorous workout.  Find a quiet spot, and simply focus on stretching.  Reach your arms overhead, gently bend to touch your toes (only as far as comfortable!), roll your shoulders, and maybe do some gentle leg stretches.  Pay attention to how your body feels with each movement. The goal is to connect with your body and release some tension, not to achieve a certain level of fitness.  Even 10 minutes can make a difference.\n",
            "\n",
            "2. **Nature's Reset (5 minutes):**  Step outside for five minutes.  Find a spot where you can sit or stand quietly, and simply observe your surroundings. Notice the colors, sounds, and smells around you.  If you have access to a green space, even better!  The shift in environment and the focus on something outside of your studies can help clear your head and reduce stress.  If you don't have time to go outside, even looking out a window can be beneficial.\n",
            "\n",
            "3. **Soothing Sensory Experience (5 minutes):**  Engage one of your senses to promote relaxation.  This could involve listening to a calming piece of music (classical, ambient, or nature sounds are good choices), diffusing a relaxing essential oil (lavender is often recommended), or gently applying a scented lotion to your hands.  The key is to choose something that feels pleasant and soothing to you. Focus on the sensory experience and let it wash over you.\n",
            "\n",
            "\n",
            "Remember, these are small steps.  Don't feel pressured to do all three today if you don't have time.  Even one of these activities will be a step in the right direction.  The most important thing is to be kind to yourself and prioritize your well-being.  If you continue to struggle, remember that seeking support from university counseling services is a sign of strength, not weakness.  You deserve to feel better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the GitHub token (replace with your actual token)\n",
        "!export GITHUB_TOKEN=your-token-here\n",
        "\n",
        "# Set the repository URL\n",
        "REPO_URL=\"https://${GITHUB_TOKEN}@github.com/ansh781/AI_for_Burnout_Prevention.git\"\n",
        "\n",
        "# Update or set the origin remote\n",
        "!git remote set-url origin $REPO_URL || git remote add origin $REPO_URL\n",
        "\n",
        "# Rename the branch to main (if needed)\n",
        "!git branch -M main\n",
        "\n",
        "# Add and commit changes (if not already done)\n",
        "!git add .\n",
        "!git commit -m \"Initial commit\" || echo \"No changes to commit\"\n",
        "\n",
        "# Push to the main branch\n",
        "!git push -u origin main\n",
        "\n",
        "# Verify the remote configuration\n",
        "!git remote -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWev3YBoZqmP",
        "outputId": "41bac71b-e10a-4f7d-9285-3ae3b2f5f4cb"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main f39fb95] Initial commit\n",
            " 21 files changed, 51025 insertions(+)\n",
            " create mode 100644 .config/.last_opt_in_prompt.yaml\n",
            " create mode 100644 .config/.last_survey_prompt.yaml\n",
            " create mode 100644 .config/.last_update_check.json\n",
            " create mode 100644 .config/active_config\n",
            " create mode 100644 .config/config_sentinel\n",
            " create mode 100644 .config/configurations/config_default\n",
            " create mode 100644 .config/default_configs.db\n",
            " create mode 100644 .config/gce\n",
            " create mode 100644 .config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n",
            " create mode 100644 .config/logs/2025.07.15/13.40.44.975775.log\n",
            " create mode 100644 .config/logs/2025.07.15/13.41.05.861388.log\n",
            " create mode 100644 .config/logs/2025.07.15/13.41.15.309812.log\n",
            " create mode 100644 .config/logs/2025.07.15/13.41.16.516678.log\n",
            " create mode 100644 .config/logs/2025.07.15/13.41.25.155140.log\n",
            " create mode 100644 .config/logs/2025.07.15/13.41.25.873076.log\n",
            " create mode 100755 sample_data/README.md\n",
            " create mode 100755 sample_data/anscombe.json\n",
            " create mode 100644 sample_data/california_housing_test.csv\n",
            " create mode 100644 sample_data/california_housing_train.csv\n",
            " create mode 100644 sample_data/mnist_test.csv\n",
            " create mode 100644 sample_data/mnist_train_small.csv\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "origin\thttps://@github.com/ansh781/AI_for_Burnout_Prevention.git (fetch)\n",
            "origin\thttps://@github.com/ansh781/AI_for_Burnout_Prevention.git (push)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save both files from Colab code cells\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(\"\"\"PASTE YOUR APP CODE HERE\"\"\")\n",
        "\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"streamlit\n",
        "langgraph\n",
        "langchain\n",
        "langchain-google-genai\n",
        "google-generativeai\"\"\")\n"
      ],
      "metadata": {
        "id": "XBw0fULcZ-HN"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"Added Streamlit frontend\"\n",
        "!git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1bVZBMta3vk",
        "outputId": "ffcd20a7-7b48-43a9-877a-4593d65a263c"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 2ec5143] Added Streamlit frontend\n",
            " 2 files changed, 2 insertions(+), 1 deletion(-)\n",
            " create mode 100644 app.py\n",
            "fatal: The current branch main has no upstream branch.\n",
            "To push the current branch and set the remote as upstream, use\n",
            "\n",
            "    git push --set-upstream origin main\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35v5ueqMa6YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Input GitHub username and Personal Access Token\n",
        "os.environ['GITHUB_USERNAME'] = input('Enter your GitHub username: ')\n",
        "os.environ['GITHUB_TOKEN'] = getpass('Enter your GitHub Personal Access Token: ')\n",
        "\n",
        "# Configure Git with your credentials\n",
        "!git config --global user.name \"{os.environ['GITHUB_USERNAME']}\"\n",
        "!git config --global user.email \"{os.environ['GITHUB_USERNAME']}@users.noreply.github.com\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBnEzWHnzWj7",
        "outputId": "2f831085-d978-47f4-90df-6b0aa42dbd5e"
      },
      "execution_count": 246,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GitHub username: ansh781\n",
            "Enter your GitHub Personal Access Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your repository URL\n",
        "repo_url = \"https://github.com/YOUR_USERNAME/YOUR_REPOSITORY.git\"\n",
        "!git clone https://{os.environ['GITHUB_USERNAME']}:{os.environ['GITHUB_TOKEN']}@github.com/ansh781/AI_for_Burnout_Prevention.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfmrkiknzXU9",
        "outputId": "0d832196-b9ee-4097-8fb9-a2c91e9c290d"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_for_Burnout_Prevention'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/39)\u001b[K\rremote: Counting objects:   5% (2/39)\u001b[K\rremote: Counting objects:   7% (3/39)\u001b[K\rremote: Counting objects:  10% (4/39)\u001b[K\rremote: Counting objects:  12% (5/39)\u001b[K\rremote: Counting objects:  15% (6/39)\u001b[K\rremote: Counting objects:  17% (7/39)\u001b[K\rremote: Counting objects:  20% (8/39)\u001b[K\rremote: Counting objects:  23% (9/39)\u001b[K\rremote: Counting objects:  25% (10/39)\u001b[K\rremote: Counting objects:  28% (11/39)\u001b[K\rremote: Counting objects:  30% (12/39)\u001b[K\rremote: Counting objects:  33% (13/39)\u001b[K\rremote: Counting objects:  35% (14/39)\u001b[K\rremote: Counting objects:  38% (15/39)\u001b[K\rremote: Counting objects:  41% (16/39)\u001b[K\rremote: Counting objects:  43% (17/39)\u001b[K\rremote: Counting objects:  46% (18/39)\u001b[K\rremote: Counting objects:  48% (19/39)\u001b[K\rremote: Counting objects:  51% (20/39)\u001b[K\rremote: Counting objects:  53% (21/39)\u001b[K\rremote: Counting objects:  56% (22/39)\u001b[K\rremote: Counting objects:  58% (23/39)\u001b[K\rremote: Counting objects:  61% (24/39)\u001b[K\rremote: Counting objects:  64% (25/39)\u001b[K\rremote: Counting objects:  66% (26/39)\u001b[K\rremote: Counting objects:  69% (27/39)\u001b[K\rremote: Counting objects:  71% (28/39)\u001b[K\rremote: Counting objects:  74% (29/39)\u001b[K\rremote: Counting objects:  76% (30/39)\u001b[K\rremote: Counting objects:  79% (31/39)\u001b[K\rremote: Counting objects:  82% (32/39)\u001b[K\rremote: Counting objects:  84% (33/39)\u001b[K\rremote: Counting objects:  87% (34/39)\u001b[K\rremote: Counting objects:  89% (35/39)\u001b[K\rremote: Counting objects:  92% (36/39)\u001b[K\rremote: Counting objects:  94% (37/39)\u001b[K\rremote: Counting objects:  97% (38/39)\u001b[K\rremote: Counting objects: 100% (39/39)\u001b[K\rremote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 39 (delta 6), reused 39 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (39/39), 8.42 MiB | 21.40 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AI_for_Burnout_Prevention/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFlQ-_WKztYo",
        "outputId": "ecdcbce5-9c1a-4084-9b19-4ca80ce3560b"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AI_for_Burnout_Prevention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL3vG3yL0HlQ",
        "outputId": "d949efdf-3772-427f-fc8b-9e6f95937302"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tmain.py  requirements.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the actual file names/paths\n",
        "!git rm *\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SElY11gT0ME0",
        "outputId": "8a560154-90c4-421a-ec35-c0cdbeb803b3"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not removing 'sample_data' recursively without -r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Delete unwanted files from repository\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGWO0YBM0Vki",
        "outputId": "51850938-d29d-4ef7-8d03-94241b0a1db2"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 62c5a06] Delete unwanted files from repository\n",
            " 2 files changed, 188 deletions(-)\n",
            " delete mode 100644 app.py\n",
            " delete mode 100644 main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYdffZVZ08iC",
        "outputId": "530110c6-485d-44cd-f572-83672c6fe3d8"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 3, done.\n",
            "Counting objects:  33% (1/3)\rCounting objects:  66% (2/3)\rCounting objects: 100% (3/3)\rCounting objects: 100% (3/3), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:  50% (1/2)\rCompressing objects: 100% (2/2)\rCompressing objects: 100% (2/2), done.\n",
            "Writing objects:  50% (1/2)\rWriting objects: 100% (2/2)\rWriting objects: 100% (2/2), 246 bytes | 246.00 KiB/s, done.\n",
            "Total 2 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/ansh781/AI_for_Burnout_Prevention.git\n",
            "   4a0e18f..62c5a06  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "LW8dlfG71Cwp",
        "outputId": "c7d55c57-0ff5-4735-a8e4-7c49f0d7f745"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1e9682b7-fede-4dd6-a8ee-a435f96eedde\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1e9682b7-fede-4dd6-a8ee-a435f96eedde\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AI_for_Burnout_Prevention.ipynb to AI_for_Burnout_Prevention.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/AI_for_Burnout_Prevention.ipynb /content/AI_for_Burnout_Prevention/AI_for_Burnout_Prevention.ipynb"
      ],
      "metadata": {
        "id": "4iRmfLtv2qMZ"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AI_for_Burnout_Prevention/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUw77VlY25dG",
        "outputId": "406ab305-5ce6-486b-f3b3-9a95d0975628"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AI_for_Burnout_Prevention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add AI_for_Burnout_Prevention.ipynb"
      ],
      "metadata": {
        "id": "jh4lJu3Y3z6T"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Add AI_for_Burnout_Prevention.ipynb to repository\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg8-ZNvJ34XT",
        "outputId": "a33642b8-48e3-4cc2-9c9b-267cbed78843"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main b387957] Add AI_for_Burnout_Prevention.ipynb to repository\n",
            " 1 file changed, 315 insertions(+)\n",
            " create mode 100644 AI_for_Burnout_Prevention.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B98A_-FF3-jp",
        "outputId": "c4fee698-f6ea-46bf-a446-4bb3e4cda863"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 4, done.\n",
            "Counting objects:  25% (1/4)\rCounting objects:  50% (2/4)\rCounting objects:  75% (3/4)\rCounting objects: 100% (4/4)\rCounting objects: 100% (4/4), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:  33% (1/3)\rCompressing objects:  66% (2/3)\rCompressing objects: 100% (3/3)\rCompressing objects: 100% (3/3), done.\n",
            "Writing objects:  33% (1/3)\rWriting objects:  66% (2/3)\rWriting objects: 100% (3/3)\rWriting objects: 100% (3/3), 2.89 KiB | 2.89 MiB/s, done.\n",
            "Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "To https://github.com/ansh781/AI_for_Burnout_Prevention.git\n",
            "   62c5a06..b387957  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-gNDzqU4AiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}